{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import pad_sequences\n",
    "import statistics\n",
    "\n",
    "\n",
    "_docEvents = 'mousedown mouseup mousemove mouseover mouseout mousewheel wheel'\n",
    "_docEvents += ' touchstart touchend touchmove deviceorientation keydown keyup keypress'\n",
    "_docEvents += ' click dblclick scroll change select submit reset contextmenu cut copy paste'\n",
    "_winEvents = 'load unload beforeunload blur focus resize error abort online offline'\n",
    "_winEvents += ' storage popstate hashchange pagehide pageshow message beforeprint afterprint'\n",
    "\n",
    "events = _docEvents.split() + _winEvents.split()\n",
    "\n",
    "labels = {\n",
    "        'survey_desktop': 0,\n",
    "        'gremlins': 1,\n",
    "        'random_mouse_bot': 2,\n",
    "        'hlisa_traces': 3,\n",
    "        'za_proxy': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Preprocessed Data\n",
    "import pickle\n",
    "pos_dict = {}\n",
    "\n",
    "with open('data/pos_dict.pickle', 'rb') as handle:\n",
    "    pos_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "# Uni-Modality Version\n",
    "\n",
    "# single_modality_pos_dict = {}\n",
    "# mouse_indices = {}\n",
    "\n",
    "# for c, values in pos_dict['event'].items():\n",
    "#     mouse_indices[c] = []\n",
    "#     for i, v in enumerate(values):\n",
    "#         if v == 2:\n",
    "#             mouse_indices[c].append(i)\n",
    "# print(len(mouse_indices))\n",
    "\n",
    "# for attr, v in pos_dict.items():\n",
    "#     single_modality_pos_dict[attr] = {}  \n",
    "#     for c, values in v.items():\n",
    "#         single_modality_pos_dict[attr][c] = []\n",
    "#         for i in mouse_indices[c]:\n",
    "#             single_modality_pos_dict[attr][c].append(values[i])\n",
    "\n",
    "# pos_dict = single_modality_pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_dict.keys())\n",
    "for k in list(labels.keys()):\n",
    "    print(k, len(pos_dict['x'][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate one_hot data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "def get_timestamp_ms(t):\n",
    "    return time.mktime(t.timetuple()) * 1000\n",
    "\n",
    "n = len(pos_dict['x'])\n",
    "n_x=0\n",
    "n_y=0\n",
    "n_action=0\n",
    "pos_dict['x_one_hot']={}\n",
    "pos_dict['y_one_hot']={}\n",
    "pos_dict['action_encoded']={}\n",
    "pos_dict['action_one_hot']={}\n",
    "pos_dict['time_diff']={}\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "all_unique_labels = set()\n",
    "for k, label in labels.items():\n",
    "    all_unique_labels.update(pos_dict['event'][k])\n",
    "encoder.fit(list(all_unique_labels))\n",
    "\n",
    "for k, label in labels.items():\n",
    "    pos_dict['action_encoded'][k]=encoder.transform(pos_dict['event'][k])\n",
    "\n",
    "for k, label in labels.items():\n",
    "    n_x=np.max([n_x,len(set(pos_dict['dir_X'][k]))])\n",
    "    n_y=np.max([n_y,len(set(pos_dict['dir_Y'][k]))])\n",
    "\n",
    "n_action=len(all_unique_labels)\n",
    "\n",
    "\n",
    "for k, label in labels.items():\n",
    "    pos_dict['x_one_hot'][k] = to_categorical(pos_dict['dir_X'][k], num_classes=n_x)\n",
    "    pos_dict['y_one_hot'][k] = to_categorical(pos_dict['dir_Y'][k], num_classes=n_y)\n",
    "    pos_dict['action_one_hot'][k] = to_categorical(pos_dict['action_encoded'][k]-1, num_classes=n_action)\n",
    "\n",
    "    \n",
    "for k, label in labels.items():\n",
    "    pos_dict['time_diff'][k]=[]\n",
    "    for i in range(len(pos_dict['action'][k])-1):\n",
    "#         if i%10000== 0:\n",
    "#             print('{}/{}'.format(i, len(pos_dict['action'][k])))\n",
    "        pos_dict['time_diff'][k]=np.append(pos_dict['time_diff'][k],get_timestamp_ms(pos_dict['t'][k][i+1])-get_timestamp_ms(pos_dict['t'][k][i]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "mousemove_index = 2\n",
    "\n",
    "def get_train_data(time_steps, data_size=5000, train_index_limit=15000, multiModal=True):\n",
    "    X, y, T = list(), list(), list()\n",
    "    es = []\n",
    "    for k, label in labels.items():\n",
    "        n = train_index_limit if len(pos_dict['x'][k]) > train_index_limit else len(pos_dict['x'][k])\n",
    "        counter=0\n",
    "        while counter < data_size:\n",
    "            x_seq = []\n",
    "            counter+=1\n",
    "            seq_start_i = random.randint(0, n - time_steps - 1)\n",
    "            if multiModal:\n",
    "                x_seq = [\n",
    "                    np.concatenate([pos_dict['action_one_hot'][k][seq_i], pos_dict['x_one_hot'][k][seq_i],\\\n",
    "                                    pos_dict['y_one_hot'][k][seq_i],[pos_dict['time_diff'][k][seq_i]]])\n",
    "                    for seq_i in range(seq_start_i, seq_start_i + time_steps)\n",
    "                ]\n",
    "            else:\n",
    "                seq_i = seq_start_i\n",
    "                while len(x_seq) < time_steps:\n",
    "                    if pos_dict['event'][k][seq_i] == mousemove_index:\n",
    "                        concat = np.concatenate([pos_dict['action_one_hot'][k][seq_i], pos_dict['x_one_hot'][k][seq_i],\\\n",
    "                                    pos_dict['y_one_hot'][k][seq_i],[pos_dict['time_diff'][k][seq_i]]])\n",
    "                        x_seq.append(concat)\n",
    "                    seq_i += 1\n",
    "                    \n",
    "            X.append(np.array(x_seq))\n",
    "            y.append(to_categorical(label, len(set(labels.values()))))        \n",
    "        \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    print(X.shape, y.shape)\n",
    "    X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def get_test_data(time_steps, time_window_ms, train_index_limit=15000, test_size=200, multiModal=True):\n",
    "    X, y, T = list(), list(), list()\n",
    "    es = []\n",
    "\n",
    "    for k, label in labels.items():\n",
    "        \n",
    "        n = len(pos_dict['x'][k])\n",
    "        try_counter = 0\n",
    "        start_index = train_index_limit if len(pos_dict['x'][k]) > train_index_limit else int(len(pos_dict['x'][k])*0.7)\n",
    "\n",
    "        for i in range(test_size):\n",
    "            diff=0\n",
    "            stop_flag = 0\n",
    "            human_counter = 0\n",
    "            while diff < time_window_ms:\n",
    "                \n",
    "                try_counter += 1\n",
    "                stop_flag +=1 \n",
    "                if stop_flag == 100:\n",
    "                    break\n",
    "                seq_start_i = random.randint(start_index, n-100)\n",
    "                x_seq = list()\n",
    "                for seq_i in range(seq_start_i, len(pos_dict['x'][k])-1):\n",
    "                    if seq_start_i > n:\n",
    "                        break\n",
    "                    if seq_i - seq_start_i == time_steps:\n",
    "                        break\n",
    "                        \n",
    "                    human_counter += 1\n",
    "\n",
    "                    diff = pos_dict['t'][k][seq_i] - pos_dict['t'][k][seq_start_i]\n",
    "                    diff = int(diff.total_seconds() * 1000.0)\n",
    "                    if diff > time_window_ms:\n",
    "                        diff = pos_dict['t'][k][seq_i-1] - pos_dict['t'][k][seq_start_i]\n",
    "                        diff = int(diff.total_seconds() * 1000.0)\n",
    "                        break\n",
    "                    \n",
    "                    x_seq.append(np.concatenate([pos_dict['action_one_hot'][k][seq_i], pos_dict['x_one_hot'][k][seq_i],pos_dict['y_one_hot'][k][seq_i],[pos_dict['time_diff'][k][seq_i]]]))\n",
    "            \n",
    "            X.append(x_seq)\n",
    "            y.append(to_categorical(label, len(set(labels.values()))))\n",
    "            T.append(diff)\n",
    "    X, y, T = np.array(X), np.array(y), np.array(T)\n",
    "    print(X.shape, y.shape, T.shape)\n",
    "    X, y, T = shuffle(X, y, T, random_state=0)\n",
    "\n",
    "    return X, y, T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, GRU, Dropout, Bidirectional\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "# n_features = len(events)+3\n",
    "n_features = n_x+n_y+n_action+1\n",
    "\n",
    "print(n_features)\n",
    "# n_features = 3\n",
    "n_steps = 50\n",
    "\n",
    "\n",
    "def train(n_steps, epochs=100, batch_size=100):\n",
    "    print('Train with Size:', n_steps)\n",
    "    X, y = get_train_data(n_steps)\n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    return_sequences=True\n",
    "\n",
    "    model.add(LSTM(300, return_sequences=return_sequences, activation='tanh', input_shape=(None, n_features)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(200, return_sequences=return_sequences, activation='tanh'))\n",
    "    model.add(LSTM(100, activation='tanh'))\n",
    "    model.add(Dense(len(set(labels.values())), activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    return model\n",
    "\n",
    "def test(model, assurance_threshold=8, test_size=200, train_index_limit=15000):\n",
    "    print('TEST ---------- Size:', n_steps)\n",
    "    time_to_detect = {}\n",
    "    confusion_matrix = {}\n",
    "    n_labels = len(list(labels.values()))\n",
    "    for k, label in labels.items():\n",
    "        n = len(pos_dict['x'][k])\n",
    "        start_index = train_index_limit if len(pos_dict['x'][k]) > train_index_limit else int(len(pos_dict['x'][k])*0.7)\n",
    "        \n",
    "        time_to_detect[k] = []\n",
    "        confusion_matrix[k] = np.zeros((n_labels, n_labels))\n",
    "        for i in range(test_size):\n",
    "            if i%(test_size/2) == 0:\n",
    "                print(k, f\"{i}/{test_size}\")\n",
    "            queue = deque([None] * assurance_threshold)\n",
    "            # print(n, n_steps)\n",
    "            \n",
    "            seq_start_i = random.randint(start_index, n-100)\n",
    "            \n",
    "            # diff = pos_dict['t'][k][seq_start_i+80] - pos_dict['t'][k][seq_start_i]\n",
    "            # diff = int(diff.total_seconds() * 1000.0)\n",
    "            # while diff > 100000:\n",
    "            #     seq_start_i = random.randint(start_index, n-100)\n",
    "            #     diff = pos_dict['t'][k][seq_start_i+100] - pos_dict['t'][k][seq_start_i]\n",
    "            #     diff = int(diff.total_seconds() * 1000.0)\n",
    "            \n",
    "            x_seq = list()\n",
    "            \n",
    "            for seq_i in range(seq_start_i, len(pos_dict['x'][k])-1):\n",
    "                if seq_start_i > n:\n",
    "                    break\n",
    "                if seq_i - seq_start_i == 200:\n",
    "                    break\n",
    "\n",
    "                x_seq.append(np.concatenate([pos_dict['action_one_hot'][k][seq_i], pos_dict['x_one_hot'][k][seq_i],pos_dict['y_one_hot'][k][seq_i],[pos_dict['time_diff'][k][seq_i]]]))                \n",
    "                y_predict = model.predict(np.array(x_seq).reshape((1, np.array(x_seq).shape[0], np.array(x_seq).shape[1])), verbose=0)\n",
    "#                     true_label = to_categorical(label, len(set(labels.values())))\n",
    "                \n",
    "                true_label = label\n",
    "                pred_label = np.argmax(y_predict[0])\n",
    "        \n",
    "                queue.popleft()\n",
    "                queue.append(pred_label)\n",
    "                \n",
    "                if all(i == list(queue)[0] for i in list(queue)):\n",
    "                    pred_label =  list(queue)[0]\n",
    "                    diff = pos_dict['t'][k][seq_i] - pos_dict['t'][k][seq_start_i]\n",
    "                    diff = int(diff.total_seconds() * 1000.0)\n",
    "                    \n",
    "                    confusion_matrix[k][true_label][pred_label] += 1\n",
    "                    \n",
    "                    correct = 0\n",
    "                    if pred_label == true_label:\n",
    "                        correct = 1\n",
    "                    time_to_detect[k].append((diff, seq_i-seq_start_i, correct))\n",
    "                    break\n",
    "\n",
    "                \n",
    "#     print(time_to_detect)\n",
    "                    \n",
    "    return time_to_detect, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "accs_no_gremlins = {}\n",
    "steps = [40, 50, 60]\n",
    "models = {}\n",
    "epochs = [50]\n",
    "for step in steps:\n",
    "    accs_no_gremlins[step] = {}\n",
    "    models[step] = {}\n",
    "    for epoch in epochs:\n",
    "        model = train(step, epoch)\n",
    "        # model.save()\n",
    "        models[step][epoch] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "results = {}\n",
    "ts = list(range(2, 20))\n",
    "confusions = {}\n",
    "for t in ts:\n",
    "    res, confusion = test(model, assurance_threshold=t, test_size=200)\n",
    "    confusions[t] = confusion\n",
    "    results[t] = copy.deepcopy(res)\n",
    "    print('------- Threshold', t)\n",
    "    accs = []\n",
    "    ttds = []\n",
    "    etds = []\n",
    "    for label, triples in res.items():\n",
    "        acc = sum([i[2] for i in triples])/len(triples)\n",
    "        accs.append(acc)\n",
    "        ttd = sum([i[0] for i in triples])/len(triples)\n",
    "        ttds.append(ttd)\n",
    "        etd = sum([i[1] for i in triples])/len(triples)\n",
    "        etds.append(etd)\n",
    "        if label == 'random_mouse_bot':\n",
    "            filler = '\\t'\n",
    "        else:\n",
    "            filler = '\\t\\t'\n",
    "        print(label, filler, f'ACC: {acc} \\t TTD: {ttd} \\t ETD: {etd}')\n",
    "    \n",
    "    acc_avg = round(sum(accs)/len(accs), 2)\n",
    "    ttd_avg = round(sum(ttds)/len(ttds), 2)\n",
    "    etd_avg = round(sum(etds)/len(etds), 2)\n",
    "\n",
    "    print(f'\\t \\t \\t acc_avg: {acc_avg}  \\t ttd_avg: {ttd_avg} \\t etd_avg: {etd_avg}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
